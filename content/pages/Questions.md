Insert questions here from [[Filtered Questions|Filtered Questions]].

- [[Model Parameters Tuning]]
#### Question prompts

[[pdoc]]
Documentation made simple. pdoc automatically generates beautiful, modern docs from your codebase. Itâ€™s perfect for keeping your projects well-documented without spending hours writing manual guides or configuring your styles.

ğ‰ğ®ğ¬ğ­ğŸğ¢ğ¥ğ
A command runner that simplifies your workflow. Instead of memorizing long commands, ğ‰ğ®ğ¬ğ­ğŸğ¢ğ¥ğ lets you define simple, reusable commands for common tasks.
related to [[Makefile]]?

ğ©ğ²ğ«ğ¢ğ ğ¡ğ­  
A static type checker thatâ€™s faster and more accurate than many alternatives. If youâ€™re using type hints (and you should be!), ğ©ğ²ğ«ğ¢ğ ğ¡ğ­ will catch errors before they happen and make your codebase way more robust.
What is the difference between pyright and [[Pydantic]]?

**Machine Learning Questions**  

[[R-squared metric not always a good indicator of model performance in regression  ]]

[[Data Cleansing]]
[[Deleting rows or filling them with the mean is not always best ]]
  
**Natural Language Processing Questions**  

[[Transformer]]
1. Why do transformer-based models like BERT or GPT outperform traditional RNNs in NLP tasks?  
[[Vector Embedding|embedding]]
1. How would you decide between using TF-IDF and Word2Vec for text vectorization?  

[[Transfer Learning]]
1. Why might fine-tuning a pre-trained model like GPT yield better results than training from scratch?  

[[Named Entity Recognition]]
2. Why is named entity recognition (NER) a challenging task, and how would you handle ambiguous entities?  
  
**Statistics Questions**  

[[Central Limit Theorem]]
1. Why is the Central Limit Theorem important for data scientists working with small sample sizes?  

[[Evaluation Metrics]]
1. How would you explain the difference between Type I and Type II errors, and why do both matter?  

[[Multicollinearity]]
1. Why does multicollinearity affect regression models, and how can techniques like PCA help? 

[[Hypothesis testing]]
1. In hypothesis testing, why might a very small p-value still lead to incorrect conclusions?  

 
**Scenario-Based Questions**  

[[Data Streaming]]
1. If youâ€™re working with a streaming dataset, why might batch processing not be suitable, and what alternatives would you consider?  

[[Regularisation]]
1. Your model is performing well on training data but poorly on validation data. Why might regularization not always solve this?  

[[Deep Learning]]
1. A client insists on using deep learning for a small dataset. Why might this backfire, and how would you explain the risks?  



[[Imbalanced Datasets]]
1. You have a perfectly balanced dataset but still experience poor classification accuracy. Why might the class separability be the issue?