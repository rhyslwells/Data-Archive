---
aliases: []
category: LANG
date modified: 1-08-2025
tags: [language_models]
---
The attention mechanism is inspired by how humans read: we donâ€™t give equal focus to every word-we ==concentrate on those most relevant to understanding the context==. Neural networks apply the same principle, dynamically weighting parts of the input sequence based on relevance.

Originally introduced to overcome the limitations of models like [[Recurrent Neural Networks]]s and [[LSTM]]s, attention mechanisms significantly improve the handling of long-range dependencies in sequence tasks. They are now central to many modern [[NLP]] applications, including machine translation, text generation, and language understanding.
### Why Attention Matters

In traditional sequence models, all information must be compressed into a single fixed-size vector, which leads to loss of context-especially for long inputs. Attention allows the model to:

* Focus selectively on relevant input tokens
* Dynamically adjust what it "attends" to at each prediction step
* Better capture dependencies across distant positions in a sequence

### How Attention Works (Simplified)

1. Score Calculation: Compute how relevant each token is to a given query token (e.g., using dot product).
2. Weighting: Apply softmax to get attention weights (a probability distribution).
3. Context Vector: Take the weighted sum of value vectors to produce a context-specific representation.

This mechanism enables the model to emphasize important tokens and de-emphasize irrelevant ones during prediction.
### See Also

* [[Self-Attention]]
* [[Self-Attention vs Multi-Head Attention]]
* [[Key Components of Attention and Formula]]
* [[Transformer]]
