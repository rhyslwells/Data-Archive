---
title: 
tags:
  - data_orchestration
aliases:
  - "#data_orchestration"
---
Data orchestration refers to the process of managing and coordinating the flow of data across various systems and environments, particularly in complex and heterogeneous cloud settings. A Data Orchestrator is responsible for modeling dependencies between different tasks, ensuring that data is processed and moved efficiently from one stage to another. It integrates with both legacy systems and modern cloud-based tools, as well as data lakes and data warehouses.

The orchestration process involves invoking computations, such as executing business logic in languages like SQL and Python, and applying machine learning models at the appropriate times. These actions can be triggered based on time schedules or custom-defined logic. Essentially, data orchestration ensures that all components of a data workflow are executed in the correct order and at the right time, facilitating seamless data integration and processing across diverse systems.

```dataview
LIST
from #data_orchestration
sort file.name asc
```

## A Data Orchestrator

A Data Orchestrator models dependencies between different tasks in [complex heterogeneous cloud environments](https://mattturck.com/data2021/) end-to-end. It handles integrations with legacy systems, new cloud-based tools, and your data lakes and data warehouses. It invokes [computation](https://en.wikipedia.org/wiki/Orchestration_(computing)), such as wrangling your business logic in [SQL](term/sql.md) and [Python](term/python.md) and applying ML models at the right time based on a time-based trigger or by custom-defined logic.

More Insights in [Data Orchestration Trends: The Shift from Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).

[[Data Pipeline to Data Products]]
