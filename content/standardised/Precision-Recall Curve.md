---
aliases:
  - PR Curve
---
A [[precision]]-[[recall]] curve is a graphical representation used to evaluate the performance of a binary classification model, particularly in scenarios where the classes are imbalanced. It plots [[precision]] (the positive predictive value) against [[recall]] (the true positive rate) for different threshold values.

Overall, precision-recall curves are a valuable tool for assessing the tradeoffs between precision and recall, helping to choose the optimal threshold for classification based on the specific requirements of the task.
### Resources

[Sklearn Link](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_precision_recall.html)
### Key Concepts:

 Precision: This metric indicates the accuracy of positive predictions. It is calculated as the ratio of true positive predictions to the total number of positive predictions (true positives + false positives).

  $\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$

 Recall ([[Sensitivity]] or True Positive Rate): This metric measures the ability of the model to identify all relevant instances. It is calculated as the ratio of true positive predictions to the total number of actual positive instances (true positives + false negatives).

  $\text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$

### Precision Recall Curve:

 Plot: The curve is generated by varying the threshold for classifying a positive instance and plotting the corresponding precision and recall values. Each point on the curve represents a precision recall pair at a specific threshold.
 Interpretation: 
 
 A model with high precision and high recall is considered to perform well. However, there is often a tradeoff between [[Precision or Recall]].

The area under the precision-recall curve (not the same as [[AUC]]) is a single scalar value that summarizes the performance of the model. A higher AUCPR indicates better model performance.

Use Cases: precision-recall curves are particularly useful in situations where the positive class is rare or when the cost of false positives and false negatives is different. They provide more insight than [[ROC (Receiver Operating Characteristic)]] curves in these scenarios because they focus on the performance of the positive class.
 
![[Pasted image 20241231172749.png]]

### Questions

What if there is more than one class?
