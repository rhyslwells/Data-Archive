---
tags: 
  - field
  - reinforcement_learning
aliases:
category: ML
phase: Model Building
topic: Reinforcement Learning Algorithms
filename: rl_algorithm_script
---
Reinforcement Learning ( [[Reinforcement learning|RL]]) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives rewards or penalties based on its actions, and its goal is to ==maximize cumulative reward.==
#### Current Research Problems

1. Sample Efficiency: Many RL algorithms require a large number of interactions with the environment to learn effectively. Research is focused on developing methods to improve sample efficiency, such as model-based approaches and transfer learning.
    
2. [[Exploration vs. Exploitation]]: Balancing exploration (trying new actions) and exploitation (choosing known rewarding actions) remains a challenge. New strategies, such as curiosity-driven learning and bandit algorithms, are being investigated.
    
3. [[Multi-Agent Reinforcement Learning]]: Extending RL to environments with multiple interacting agents introduces complexity in learning optimal strategies. Research includes coordination, competition, and communication between agents.
    
4. Robustness and Stability: Ensuring that RL agents perform reliably in changing or adversarial environments is a key area of study. Techniques for robust control and stability analysis are being explored.

### Algorithms in Reinforcement Learning

[[Q-Learning]]

[[Sarsa]]
#### Components

- Agent: An entity that interacts with the environment and learns to optimize its actions based on rewards.
- State ($s$): The current situation in the environment, often defined by the positions and attributes relevant to the agent's decision-making.
- Action ($a$): The available moves or decisions an agent can take.
- Reward ($r$): A scalar value received after taking an action, representing feedback from the environment.
- [[Policy]] ($\pi$): A strategy that the agent follows, mapping states to actions.
- Q-Value ($Q(s, a)$): The expected cumulative reward for taking a particular action in a given state and following the policy thereafter. The Q-values guide the agent in making decisions that maximize long-term rewards. [[Q-Learning]]
#### Mathematical Foundations

- [[Markov Decision Processes]] ([[Markov Decision Processes|MDP]]s): The mathematical framework for modelling decision-making, characterized by states, actions, transition probabilities, and rewards. Your understanding of probability theory and stochastic processes will be crucial here.
- Dynamic Programming: Techniques such as [[Bellman Equations]] equations are central to RL, as they provide a way to break down complex decision-making problems into simpler subproblems.
- Optimization Techniques: Finding optimal [[Policy|polices]] often involves advanced optimization methods, including gradient ascent and policy iteration.

# Reinforcement Learning

Use import gym

Action Space: What actions are available to the agent?
Observation Space: What information is available to the agent?
Reward Envioronment: What rewards can be given to the agent?

Loads the enviroemnt for examples
env = gym.make('LunarLander-v2')

Can step though the environment dynamics
next_state, reward, done, info = env.step(action)

```python
# Select an action
action = 0

# Run a single time step of the environment's dynamics with the given action.
next_state, reward, done, info = env.step(action)

with np.printoptions(formatter={'float': '{:.3f}'.format}):
    print("Initial State:", initial_state)
    print("Action:", action)
    print("Next State:", next_state)
    print("Reward Received:", reward)
    print("Episode Terminated:", done)
    print("Info:", info)
```