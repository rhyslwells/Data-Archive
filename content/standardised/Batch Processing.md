---
title: 
tags:
  - data_orchestration
aliases: 
category:
---
>[!Summary]
> **Batch Processing** is a technique used to handle and process large datasets efficiently. It works by breaking the data into smaller chunks and processing them together in a single batch.

[[Apache Spark]] is the leading technology for batch processing, offering scalable and distributed data processing. It can handle unmanageable data sizes by using parallelism and [[Distributed Computing]]

A key concept in batch processing is **MapReduce**:
  - **Map**: Splits the data into smaller, manageable pieces for parallel processing.
  - **Reduce**: Aggregates the processed data results from the individual tasks.
  - **Order**: The order of Map and Reduce steps is flexible; the primary focus is on splitting and then aggregating data.

Batch processing is widely supported by cloud infrastructures like **Amazon EMR** and **[[Databricks]]**, which provide scalable environments for running batch jobs.
