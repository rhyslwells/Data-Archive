---
type: 
tags:
  - "#ml"
  - "#ml_optimisation"
  - transformation
---
Feature Preprocessing is when you clean the data first. Filling in blanks ect only. No dealing with outliers.

In data science, feature preprocessing refers to the process of transforming raw data into a clean and usable format for machine learning models. This step is crucial for improving model performance and ensuring accurate predictions. Key aspects of feature preprocessing include:

1. Data Cleaning: Removing or imputing missing values, correcting errors, and handling outliers to ensure data quality.

2. **[[Feature Scaling]]**: Normalizing or standardizing features to ensure they are on a similar scale. Normalization and Scaling: Adjusting the range of features, often using techniques like min-max scaling or z-score normalization, to ensure that all features contribute equally to the model.

3. [[Encoding Categorical Variables]]: Converting categorical data into numerical format using methods like one-hot encoding or label encoding.

4. [[Feature Selection]]: Identifying and retaining the most relevant features that contribute to the predictive power of the model, often using statistical tests or model-based approaches.

5. [[Dimensionality Reduction]]: Reducing the number of features while preserving important information, using techniques like Principal Component Analysis (PCA).

6. [[Feature Engineering]]: Creating new features from existing data to improve model performance, often based on domain knowledge.