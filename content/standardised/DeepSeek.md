---
tags: 
aliases: 
category: 
phase: 
topic: 
filename:
---
[[LLM]] example 

open source

optimising for preformance vs efficency.

Deepseek leading in efficency

o3 mini

[[Chain of thought]]
can see it - ui choice

[[Distillation]] - use gpt output and trains on it. 

[[Data Security]]

#drafting 

[[jevon paradox]] - as cost decreases usage increases 

edge inference [[Edge Machine Learning Models]]

[The Genius of DeepSeekâ€™s 57X Efficiency Boost](https://www.youtube.com/watch?v=0VLAoVGf_74)
key value caching 
impacts attention block compute scaling: linear
increased memory usage
Solution: 
multi-query attention vs mutli - head attention
Grouped-query attention
Multi-head latent attention - deepseek uses
uses compresses latent space
linear algebra 
absorbed weights at training




- **Access to Advanced AI Features Without Payment**: You can now access powerful reasoning models like DeepSeek's R1 and ChatGPT's 03 Mini for free. These models are good at complex math, programming, and step-by-step reasoning.

- **Privacy Protection**: If you are concerned about privacy, you have options to protect your data by using platforms like Perplexity, Venice AI, or Cursor to access DeepSeek models, which keeps data in the US. For full privacy, you can run DeepSeek models locally using LM Studio or oLama, but this may limit access to more powerful models due to hardware constraints.

- **Smart Choices About Switching**: Consider if DeepSeek offers clear advantages for your specific needs before changing your current AI tools or workflows. DeepSeek is beneficial for developers focused on cost minimization. If you are an everyday user already paying for ChatGPT and concerned about data storage, switching may not be necessary unless DeepSeek significantly improves your workflow.