---
title: 
tags:
  - drafting
  - language_models
aliases: 
category:
---
A word embedding is a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors capture ==semantic meanings and relationships between words based on their context in large text corpora.== Key characteristics of word embeddings include:

1. [[Dimensionality Reduction]]: Words are represented in a lower-dimensional space compared to traditional methods like one-hot encoding, which results in more efficient computations.

2. Semantic Relationships: Words with similar meanings or contexts are located close to each other in the vector space. For example, "king" and "queen" might be closer to each other than "king" and "apple."

![[Pasted image 20241015211934.png]]

4. ==Contextual Understanding:== Word embeddings capture the context in which words appear, allowing models to understand nuances and relationships in language.

Popular methods for generating word embeddings include Word2Vec, GloVe, and FastText. These embeddings are widely used in natural language processing tasks to improve the performance of models by providing them with a richer understanding of language.

[[t-SNE]]

![[Pasted image 20241015211844.png]]





