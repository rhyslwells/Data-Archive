---
title: 
tags:
  - language_models
aliases:
  - LLMs
  - Large Language Models
category:
---
A Large Language Model (LLM) is a type of language model designed for language understanding and generation. They can perform a variety of tasks, including:

- Text generation
- Machine translation
- Summary writing
- Image generation from text
- Machine coding
- Chatbots or Conversational AI
# Questions

- [[How do we evaluate of LLM Outputs]]
- [[LLM Memory|What is LLM memory]]
- [[Relationships in memory|Managing LLM memory]]
- [[Mixture of Experts]]: having multiple experts instead of one big model.
- [[Distillation]]
- Mathematics on the parameter usage [[Attention mechanism]]
- Use of [[Reinforcement learning]] in training [[Chain of thought]] methods in LLM's (deepseek)

## How do Large Language Models (LLMs) Work?

Large [[Language Models]] (LLMs) are a type of artificial intelligence model that is designed to understand and generate human language. Key aspects of how they work include:

- Word Vectors: LLMs represent words as long lists of numbers, known as word vectors ([[standardised/Vector Embedding|word embedding]]).
- Neural Network Architecture: They are built on a neural network architecture known as the [[Transformer]]. This architecture enables the model to identify relationships between words in a sentence, irrespective of their position in the sequence.
- [[Transfer Learning]]: LLMs are trained using a technique known as transfer learning, where a pre-trained model is adapted to a specific task.

## Characteristics of LLMs

- ==Non-Deterministic:== LLMs are non-deterministic, meaning the types of problems they can be applied to are of a probabilistic nature (==temperature==).
- Data Dependency: The performance and behaviour of LLMs are heavily influenced by the data they are trained on.


