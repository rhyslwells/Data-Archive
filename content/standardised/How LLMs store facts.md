---
aliases: 
category: 
date modified: 27-07-2025
tags:
  - NLP
  - agents
---
[How might LLMs store facts](https://www.youtube.com/watch?v=9-Jl0dxWQs8&list=PLZx_FHIHR8AwKD9csfl6Sl_pgCXX19eer&index=6)

Not solved

How do [[Multilayer Perceptrons]] store facts?

Different directions encode information in [[Vector Embedding]] space.

MLP's are blocks of vectors, these are acted on my the context matrix 

[[Johnsonâ€“Lindenstrauss lemma]]

Sparse Autoencoder - used in [[interpretability]] of [[LLM]] responses

See [[Anthropic]] posts
- https://transformer-circuits.pub/2022/toy_model/index.html#adversarial
- https://transformer-circuits.pub/2023/monosemantic-features