---
tags:
  - "#flashcards"
category:
---
Pull flashcard files to phone and tablets with obsidian. Or with [[Anki]].

## Interviews

Tell about a recent project of yours {{c1::Collaborating in the image matching Kaggle competition. Obviously S2DS project too.}}  
What are some areas in this business you are interested in? {{c1::Technical consulting}}, {{c2::energy projects}}, {{c3::any area with room to problem solve}}, {{c4::apply the scientific method to business problems}}, {{c5::areas where data can be turned into decisions}}, {{c6::building technical systems for operations (feasibility projects)}}  
How do you approach prioritizing tasks in a data science project? {{c1::Current project objectives}}, {{c2::complexity}}, {{c3::dependencies}}, {{c4::client impact}}  
How do you handle conflicts or disagreements within a data science team? {{c1::Promote open communication}}, {{c2::facilitate discussions}}, {{c3::seek win-win solutions to resolve conflicts constructively}}  
How do you stay updated with the latest developments and trends in data science? {{c1::Attend events}}, {{c2::talk to peers}}, {{c3::stay connected to colleagues online}}, {{c4::watch updates on YouTube}}  
How do you ensure the quality and reliability of data used in your data science projects? {{c1::Implement data validation and cleaning procedures}}, {{c2::conduct exploratory data analysis}}, {{c3::collaborate with domain experts and engineers to verify data accuracy and relevance}}  
Can you describe a time when you had to make a decision under tight deadlines in a data science project? {{c1::In S2DS, the client wanted further features to be added, so we said no nicely}}  
What strategies do you employ to ensure alignment between data science initiatives and business objectives? {{c1::Collaborate closely with stakeholders to understand business goals}}, {{c2::prioritize projects based on their strategic importance}}, {{c3::regularly communicate progress and results to ensure alignment and maximize impact}}  
How do you stay organized and manage deadlines in your data science projects? {{c1::Utilize project management tools}}, {{c2::break down tasks into manageable components}}, {{c3::set realistic timelines}}, {{c4::regularly reassess priorities to ensure timely delivery}}  
Can you discuss a challenging problem you encountered in a data science project and how you resolved it? {{c1::In Alice in Wonderland people issue}}, {{c2::analyzed the root cause}}, {{c3::consulted domain experts or literature}}, {{c4::experimented with alternative approaches}}, {{c5::iteratively refined solutions until achieving satisfactory results}}  
What do you think is the most important thing in a team? {{c1::Buy-in}}, {{c2::communication}}, {{c3::initiative}}  
What do you think is a no-go in a team? {{c1::Lack of accountability}}, {{c2::blaming people}}, {{c3::own your mistakes and learn from them}}  
What are some areas of the DS field you are interested in? {{c1::NLP}}, {{c2::time series analysis}}  
Why are you interested in data science? {{c1::Problem solving aspect with technically interesting tools}}, {{c2::work with technically minded people}}, {{c3::enjoy the scientific viewpoint}}  
How would you interact with the data science community? {{c1::Participate in Datafest}}, {{c2::Kaggle projects}}, {{c3::engage with colleagues}}  
What is data normalization and why do we need it? {{c1::Data Normalised Schema is used in preprocessing as it rescales values to fit within a specific range}}  
Explain dimensionality reduction, where it’s used, and its benefits? {{c1::Dimensionality reduction reduces the number of feature variables by obtaining a set of principal variables}}, {{c2::reduces storage space}}, {{c3::speeds up computation}}, {{c4::removes redundant features}}, {{c5::enables data visualization to identify patterns}}  
How do you handle missing or corrupted data in a dataset? {{c1::Drop affected rows or columns}}, {{c2::replace them with another value}}, {{c3::fill with a placeholder using methods like isnull(), dropna(), fillna() in Pandas}}  
How would you go about doing an exploratory data analysis (EDA)? {{c1::Gain insights from data before predictive models}}, {{c2::start with high-level insights}}, {{c3::drop unnecessary columns}}, {{c4::fill missing values}}, {{c5::create visualizations (bar plots, scatter plots)}}  
How do you know which machine learning model you should use? {{c1::Model selection depends on problem nature, data characteristics, desired outcomes, and often involves trial-and-error}}  
Explain your PhD and its outcomes. {{c1::Looked for counterexamples}}, {{c2::defined FCJ}}, {{c3::built algorithms to compute them}}, {{c4::computed them}}  
What is the difference between Type I vs Type II error? {{c1::Type I error: null hypothesis true but rejected}}, {{c2::Type II error: null hypothesis false but not rejected}}  
What is linear regression? {{c1::A statistical method to model the relationship between a dependent variable and one or more independent variables}}  
What do the terms P-value, coefficient, R-squared value mean? {{c1::P-value indicates coefficient significance}}, {{c2::coefficient shows strength and direction of relationship}}, {{c3::R-squared measures proportion of variance explained}}  
What are the assumptions required for linear regression? {{c1::Linear relationship}}, {{c2::normally distributed errors}}, {{c3::independent errors}}, {{c4::minimal multicollinearity}}  
What is a statistical interaction? {{c1::When the effect of one variable on a dependent variable depends on the value of another variable}}  
What is selection bias? {{c1::Systematic sampling error}}, {{c2::sample not representative of population}}, {{c3::leads to incorrect conclusions about the population}}
## Data Engineering

### [[Relating Tables Together]]
- A [[Primary Key]] uniquely identifies each record in a table, {{ensuring that no two records can have the same key}}.
- A [[Foreign Key]] establishes {{a link}} between two tables, allowing for relationships to be maintained and ensuring data integrity.
- One-to-Many relationships allow {{a single record in Table A to relate to multiple records in Table B}}.
- Junction tables are used to manage {{many-to-many relationships}} by containing {{foreign keys}} from both tables it connects.

### [[Database Schema]]
- A [[Database Schema]] defines how data is organized in a database, specifying {{tables, columns, relationships, and constraints}}.
- The importance of a schema lies in its ability to ensure that data is stored {{consistently and can be queried efficiently}}. A well-defined schema helps in organizing data, making it {{manageable and meaningful}}.
### [[Data Principles]]
- Data principles ensure that data is managed, used, and maintained {{effectively and ethically}}.
- High-quality data is crucial for making {{informed decisions}} and involves ensuring {{data accuracy, completeness, and reliability}}.
- Data governance establishes {{clear policies and procedures for data management}}, ensuring compliance with regulations and data integrity.
- Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically, including aspects like {{Data Quality, Data Governance, and Data Privacy}}.

### [[Data Modelling]]
- Data modelling is the process of creating a {{visual representation}} of a system's data and the relationships between different data elements.
- The workflow of data modeling includes a {{conceptual model, logical model, and physical model}} to ensure data is logically structured and organized.

## Project Management

### [[Jobs to be done]]
- Understanding pains and gains helps you design solutions that reduce {{pains and create gains}}.

### [[Asking questions]]
- A good question is characterized by being {{purposeful, contextual, open/expansive, challenging, precise, and sequenced}}.
- Asking better questions enhances {{thinking, learning, problem-solving, and communication}}.
### [[Communication principles]]
- The main goal of effective communication is to {{enhance clarity and engagement}} in discussions.
    

### [[Change Management]]
- Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a {{desired future state}}.
- Effective change management helps {{minimize resistance, improves engagement,}} and {{increases the likelihood of successful outcomes}}.


### [[1-to-1's with a Line Manager]]
- The purpose of regular 1-to-1s is to ensure {{alignment, feedback, support, and professional growth}}.

### [[Building a data team]]
- A good data team understands and influences {{business priorities}} and is evaluated on their impact to the whole business rather than just outputs.

### [[Documentation & Meetings]]
- Documentation templates help {{standardize processes}} for meetings, ensuring clear communication and follow-up.
### [[Software Development Life Cycle]]
- The Software Development Life Cycle (SDLC) comprises several phases, including {{Planning and Analysis, Designing, Development, Testing, Deployment, and Maintenance}}. Agile methodologies focus on {{flexibility and continuous delivery}}, integrating testing throughout the development process.

## Business

### [[Operational Resilience for Growth and Adaptability]]
- Operational resilience involves preparation, foundational planning, and adaptability to {{future disruptions}} such as economic, regulatory, and climate changes. Resilience creates a {{competitive advantage}} by allowing businesses to seize emerging opportunities while others falter. Key practices for operational resilience include {{continuity and recovery planning}} embedded in daily operations. Trust-building is essential; people must understand and value the {{role they play}} in resilience efforts.

### [[Digital Transformation]]
- Digital transformation aims to provide {{innovation to business processes}} and save time, do more, and save money.
- To digitally transform your department, you'll need to approach the process in a {{structured and strategic way}}.
- Digital transformation should not only address today’s inefficiencies but also prepare digital assets and infrastructure for {{future automation, analytics, and growth}}.
### [[Business observability]]
- Business observability refers to the ability to gain insights into the internal state and performance of a business through {{continuous monitoring and analysis of data}}.
- Key components of business observability include {{data collection}}, {{monitoring, analysis, and feedback loops}}. Effective business observability helps organizations {{detect issues early}} and optimize operations.
## ML

### [[NLP]]
- In NLP the [[Bag of Words]] model represents text data by counting the {{occurrence of each word}} in a document. TF-IDF stands for {{Term Frequency-Inverse Document Frequency}} and improves on Bag of Words by considering the importance of a word in a document relative to its frequency across multiple documents.
- One-hot encoding converts categorical data into a {{binary vector representation}}.

### [[nltk]]
- NLTK (Natural Language Toolkit) is a Python library for working with {{human language data}}. It provides tools for text processing, linguistic analysis, and building natural language processing ([[NLP]]) models. Key features of NLTK include {{tokenization}}, {{stopwords removal}}, and {{Part of Speech Tagging}}.

### [[Named Entity Recognition]]
- Named Entity Recognition (NER) is a subtask of NLP that involves identifying and classifying key entities in text into {{predefined categories}} such as names, organizations, and locations.

### [[Prompt Engineering]]
- Prompt engineering is a technique in NLP that involves designing and optimizing {{input prompts}} to get the most relevant and accurate responses from large language models (LLMs). Techniques like {{prompt retrievers}} enhance the ability to retrieve and generate contextually appropriate prompts. Quality prompts can reduce {{ambiguity}} and guide model outputs.

### [[BERT]]
- BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art NLP model developed by {{Google}}. It utilizes a {{bidirectional approach}} to capture the context of a word based on both its left and right surroundings in a sentence. BERT is pre-trained on a large corpus using {{Masked Language Modeling}} and {{Next Sentence Prediction}}.

### [[Transformer]]
- A transformer is a deep learning model architecture designed to process {{sequential data}} like natural language. The core innovation of transformers is the {{self-attention mechanism}}, which allows the model to weigh the importance of different words in a sentence relative to each other. Transformers can process entire sequences in {{parallel}}, making them more efficient than {{RNNs}}.

### [[Attention mechanism]]
- The attention mechanism allows a model to focus on {{different parts of an input sequence}} when making predictions. In its simplest form, attention assigns {{weights}} to each input token based on its relevance to the current output token being generated.

### [[Backpropagation]]
- Backpropagation is an algorithm for training neural networks by iteratively correcting {{prediction errors}}. It calculates the gradient of the loss function with respect to each model parameter, enabling updates via {{Gradient Descent}}. The chain rule from calculus is employed to propagate {{errors backward}} through the network.
### [[Vector Embedding]]
- Vector embedding is a technique used to represent data in a {{continuous vector space}}. In the embedding space, similar items are positioned {{close to each other}}. Popular methods for generating vector embeddings include {{Word2Vec}}, GloVe, and FastText.

### [Feature Extraction.md](obsidian://open?vault=content&file=standardised%2FFeature%20Extraction.md)

- Feature extraction is the process of transforming raw data into a {{structured set of informative features}} that are most relevant to a machine learning task. A core strategy in feature extraction is {{Dimensionality Reduction}}, which compresses data while preserving its most important variance.
### [[Machine Learning Algorithms]]
- Machine learning algorithms are used to {{automate tasks, extract insights, and make informed decisions}}. Choosing the right algorithm involves understanding the {{task, characteristics of the data, and strengths and limitations of different algorithms}}.
- Common classification algorithms include {{Logistic Regression, Support Vector Machines, and Decision Trees}}. Common regression algorithms include {{Linear Regression and Support Vector Regression}}.


### [[Model Optimisation]]
- Model optimization aims to enhance a model's performance by fine-tuning its {{parameters and hyperparameters}}. The goal of model optimization is to improve the model's {{accuracy, efficiency, and ability to generalize}} to new data. Techniques like {{grid search}} can be used for hyperparameter tuning.

- Evaluating a models using appropriate metrics is crucial for {{detecting overfitting and providing reliable performance estimates}}. A technique to assess the model's performance by splitting the data into multiple subsets for training and testing is called {{Cross Validation}}. Metrics for classification include {{accuracy, precision, recall, F1-score}}, and for regression include {{Mean Absolute Error (MAE), Mean Squared Error (MSE)}}.

### [[Machine Learning Operations]]
- MLOps emphasizes the importance of {{monitoring and maintaining models}} over time.
- Continuous monitoring of model performance is crucial in MLOps, as it helps identify when a model needs {{retraining}} due to changes in data patterns, and data drift.

### [Supervised Learning.md](obsidian://open?vault=content&file=standardised%2FSupervised%20Learning.md)

- Supervised learning is a type of machine learning where an algorithm learns from {{labeled data}} to make predictions or decisions.

### [Classification.md](obsidian://open?vault=content&file=standardised%2FClassification.md)

- Classification is a type of supervised learning in machine learning, where the algorithm learns from {{labeled data}} to predict which {{category or class a new, unlabeled data point belongs to.}} The goal is to assign the correct label to input data based on patterns learned from the training set. {{Decision Trees}} is a type of classification algorithm.
### [Overfitting.md](obsidian://open?vault=content&file=standardised%2FOverfitting.md)

- Overfitting in machine learning occurs when a model captures {{not only the underlying patterns in the training data but also the noise}}, leading to poor performance on unseen data. Overfitting indicates high variance in the model’s performance, which can be identified by a significant {{drop in accuracy}} between {{training and test datasets}}.

### [Bagging.md](obsidian://open?vault=content&file=standardised%2FBagging.md)

- Bagging, short for Bootstrap Aggregating, is an {{ensemble technique}} designed to improve the stability and accuracy of machine learning algorithms. It works by {{training multiple instances of the same learning algorithm on different subsets of the training data and then combining their predictions}}. A well-known example of a bagging technique is {{the Random Forest}} algorithm.

### [Gradient Boosting.md](obsidian://open?vault=content&file=standardised%2FGradient%20Boosting.md)

- Gradient Boosting is a technique used for building predictive models, particularly in tasks like regression and classification. It combines the concepts of boosting and gradient descent to create strong models by {{sequentially combining multiple weak learners.}} Each new model focuses on {{correcting the mistakes made by the previous ones}} by fitting to the residuals.

### [Cross Validation.md](obsidian://open?vault=content&file=standardised%2FCross%20Validation.md)

- Cross-validation is a statistical technique used in machine learning to assess how well a model will {{generalize to an independent dataset.}}
- Cross-validation involves dividing the dataset into equal-sized subsets (folds) and using each fold as a {{validation}} set once, while the remaining  folds are used for {{training}}.
- With cross validation the model's performance is averaged across all  folds to provide a more robust {{estimate of its generalization performance.}}


## Statistics

### [[Statistics]]

- In hypothesis testing, the {{null hypothesis}} ($H_0$) states that there is no effect or difference.
- A small p-value (typically {{< 0.05}}) suggests that we should reject the null hypothesis. A small p-value indicates that the observed effect is {{statistically significant}}.
- The Central Limit Theorem states that the distribution of the sum of a large number of independent random variables approaches a {{normal distribution}}.
- Statistics is the science of collecting, analyzing, interpreting, presenting, and organizing data to understand the world, which is made of {{probabilities}}.

### [[Hypothesis testing]]
- In hypothesis testing, the {{null hypothesis}} ($H_0$) is tested against the alternative hypothesis ($H_1$). Accepting the null hypothesis means there is {{insufficient evidence}} to support the alternative hypothesis.
- The T-test is an example of a statistical test used to compare the {{means of two groups}}.

### [[Covariance]]
- Covariance measures the degree to which two random variables {{change together}}. A positive covariance indicates that as one variable increases, the other variable tends to {{also increase}}.

### [[Standard deviation]]
- Standard deviation quantifies the amount of {{variation or dispersion}} in a set of data values.
- A small standard deviation indicates that data points are {{close to the mean}}.

### [[Covariance Structures]]
- A covariance structure describes how variability and relationships between {{variables}} are modeled in a dataset.
- The covariance matrix shows the variance of each variable along the {{diagonal}} and the covariances between variables off the diagonal.
- Covariance structures are crucial for multivariate statistical methods like {{principal component analysis}} and regression.

### [[Statistics]]
- Statistics is the science of collecting, analyzing, interpreting, presenting, and organizing data to understand the world, which is made of {{probabilities}}.

### [[Distributions]]
- Discrete distributions have probabilities concentrated on specific values, while {{continuous distributions}} have probabilities spread over a continuous range.

### [[Statistical Tests]]
- Statistical tests are methods used to determine if there is a {{significant difference}} between groups or if a relationship exists between variables.

### [[Statistical Assumptions]]
- The assumption of normality posits that the data follows a {{normal distribution}}, which is crucial for many statistical tests.

### [[Hypothesis testing]]
- In hypothesis testing, the null hypothesis ($H_0$) states that there is {{no effect or no difference}}.

### [[Regression]]
- Linear regression assumes a {{linear relationship}} between the dependent variable and independent variables, aiming to {{minimize the residual sum of squares}}.




























