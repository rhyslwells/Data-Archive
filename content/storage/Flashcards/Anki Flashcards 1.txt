A Primary Key uniquely identifies each record in a table, {{c1::ensuring that no two records can have the same key}}.  
A Foreign Key establishes {{c1::a link}} between two tables, allowing for relationships to be maintained and ensuring data integrity.  
One-to-Many relationships allow {{c1::a single record in Table A to relate to multiple records in Table B}}.  
Junction tables are used to manage {{c1::many-to-many relationships}} by containing {{c2::foreign keys}} from both tables it connects.  
A Database Schema defines how data is organized in a database, specifying {{c1::tables, columns, relationships, and constraints}}.  
The importance of a schema lies in its ability to ensure that data is stored {{c1::consistently and can be queried efficiently}}. A well-defined schema helps in organizing data, making it {{c2::manageable and meaningful}}.  
Data principles ensure that data is managed, used, and maintained {{c1::effectively and ethically}}.  
High-quality data is crucial for making {{c1::informed decisions}} and involves ensuring {{c2::data accuracy, completeness, and reliability}}.  
Data governance establishes {{c1::clear policies and procedures for data management}}, ensuring compliance with regulations and data integrity.  
Data principles are essential for ensuring that data is managed, used, and maintained effectively and ethically, including aspects like {{c1::Data Quality, Data Governance, and Data Privacy}}.  
Data modelling is the process of creating a {{c1::visual representation}} of a system's data and the relationships between different data elements.  
The workflow of data modeling includes a {{c1::conceptual model, logical model, and physical model}} to ensure data is logically structured and organized.  
Understanding pains and gains helps you design solutions that reduce {{c1::pains and create gains}}.  
A good question is characterized by being {{c1::purposeful, contextual, open/expansive, challenging, precise, and sequenced}}.  
Asking better questions enhances {{c1::thinking, learning, problem-solving, and communication}}.  
The main goal of effective communication is to {{c1::enhance clarity and engagement}} in discussions.  
Change management is a structured approach to transitioning individuals, teams, and organizations from a current state to a {{c1::desired future state}}.  
Effective change management helps {{c1::minimize resistance, improves engagement,}} and {{c2::increases the likelihood of successful outcomes}}.  
The purpose of regular 1-to-1s is to ensure {{c1::alignment, feedback, support, and professional growth}}.  
A good data team understands and influences {{c1::business priorities}} and is evaluated on their impact to the whole business rather than just outputs.  
Documentation templates help {{c1::standardize processes}} for meetings, ensuring clear communication and follow-up.  
The Software Development Life Cycle (SDLC) comprises several phases, including {{c1::Planning and Analysis, Designing, Development, Testing, Deployment, and Maintenance}}. Agile methodologies focus on {{c2::flexibility and continuous delivery}}, integrating testing throughout the development process.  
Operational resilience involves preparation, foundational planning, and adaptability to {{c1::future disruptions}} such as economic, regulatory, and climate changes. Resilience creates a {{c2::competitive advantage}} by allowing businesses to seize emerging opportunities while others falter. Key practices for operational resilience include {{c3::continuity and recovery planning}} embedded in daily operations. Trust-building is essential; people must understand and value the {{c4::role they play}} in resilience efforts.  
Digital transformation aims to provide {{c1::innovation to business processes}} and save time, do more, and save money.  
To digitally transform your department, you'll need to approach the process in a {{c1::structured and strategic way}}.  
Digital transformation should not only address todayâ€™s inefficiencies but also prepare digital assets and infrastructure for {{c1::future automation, analytics, and growth}}.  
Business observability refers to the ability to gain insights into the internal state and performance of a business through {{c1::continuous monitoring and analysis of data}}.  
Key components of business observability include {{c1::data collection}}, {{c2::monitoring, analysis, and feedback loops}}. Effective business observability helps organizations {{c3::detect issues early}} and optimize operations.  
In NLP the Bag of Words model represents text data by counting the {{c1::occurrence of each word}} in a document. TF-IDF stands for {{c2::Term Frequency-Inverse Document Frequency}} and improves on Bag of Words by considering the importance of a word in a document relative to its frequency across multiple documents.  
One-hot encoding converts categorical data into a {{c1::binary vector representation}}.  
NLTK (Natural Language Toolkit) is a Python library for working with {{c1::human language data}}. Key features of NLTK include {{c2::tokenization}}, {{c3::stopwords removal}}, and {{c4::Part of Speech Tagging}}.  
Named Entity Recognition (NER) involves identifying and classifying key entities in text into {{c1::predefined categories}} such as names, organizations, and locations.  
Prompt engineering involves designing and optimizing {{c1::input prompts}}. Techniques like {{c2::prompt retrievers}} enhance retrieval. Quality prompts can reduce {{c3::ambiguity}} and guide model outputs.  
BERT is developed by {{c1::Google}}. It uses a {{c2::bidirectional approach}} and is pre-trained with {{c3::Masked Language Modeling}} and {{c4::Next Sentence Prediction}}.  
A transformer processes {{c1::sequential data}} using the {{c2::self-attention mechanism}}, and processes sequences in {{c3::parallel}}, making it more efficient than {{c4::RNNs}}.  
The attention mechanism allows focusing on {{c1::different parts of an input sequence}} by assigning {{c2::weights}} to each input token.  
Backpropagation corrects {{c1::prediction errors}} via {{c2::Gradient Descent}}, using the chain rule to propagate {{c3::errors backward}}.  
Vector embedding represents data in a {{c1::continuous vector space}}, where similar items are {{c2::close to each other}}. Methods include {{c3::Word2Vec}}, GloVe, and FastText.  
Feature extraction transforms raw data into a {{c1::structured set of informative features}}. Dimensionality Reduction {{c2::compresses data while preserving variance}}.  
Machine learning algorithms are used to {{c1::automate tasks, extract insights, and make informed decisions}}. Choosing involves understanding {{c2::the task, data characteristics, and algorithm strengths/limitations}}.  
Classification algorithms include {{c1::Logistic Regression, Support Vector Machines, and Decision Trees}}. Regression algorithms include {{c2::Linear Regression and Support Vector Regression}}.  
Model optimization fine-tunes {{c1::parameters and hyperparameters}} to improve {{c2::accuracy, efficiency, and generalization}}. Grid search is a common method.  
Evaluation detects {{c1::overfitting}} and uses {{c2::Cross Validation}}. Metrics include {{c3::accuracy, precision, recall, F1-score}}, {{c4::MAE, MSE}}.  
MLOps emphasizes {{c1::monitoring and maintaining models}}. Continuous monitoring identifies when to {{c2::retrain}} due to data drift.  
Supervised learning learns from {{c1::labeled data}}.  
Classification predicts {{c1::category/class}} from {{c2::labeled data}}. {{c3::Decision Trees}} are one method.  
Overfitting captures {{c1::patterns and noise}}, causing {{c2::drop in accuracy}} between {{c3::training and test datasets}}.  
Bagging is an {{c1::ensemble technique}} that {{c2::trains multiple algorithm instances on data subsets}}. {{c3::Random Forest}} is an example.  
Gradient Boosting {{c1::sequentially combines weak learners}} to {{c2::correct mistakes of previous models}}.  
Cross-validation assesses {{c1::generalization}}, using {{c2::validation}} and {{c3::training}} folds, averaging results for {{c4::robust estimates}}.  
In hypothesis testing, the {{c1::null hypothesis}} states no effect or difference.  
A p-value {{c1::< 0.05}} suggests rejecting the null; it means the effect is {{c2::statistically significant}}.  
The Central Limit Theorem says sums of large numbers of independent variables approach a {{c1::normal distribution}}.  
Statistics involves understanding the world through {{c1::probabilities}}.  
In hypothesis testing, accepting the null means {{c1::insufficient evidence}} for the alternative.  
A T-test compares the {{c1::means of two groups}}.  
Covariance measures how variables {{c1::change together}}; positive means {{c2::both increase}}.  
Standard deviation measures {{c1::variation}}; small means {{c2::close to the mean}}.  
A covariance structure models {{c1::variability and relationships}}.  
The covariance matrix has variances on the {{c1::diagonal}} and covariances off-diagonal.  
Covariance structures are key for {{c1::principal component analysis}} and regression.  
Discrete distributions have probabilities at specific values; {{c1::continuous distributions}} are spread continuously.  
Statistical tests determine if {{c1::significant differences}} exist between groups or variables.  
The normality assumption means data follows a {{c1::normal distribution}}.  
In hypothesis testing, the null says {{c1::no effect or no difference}}.  
Linear regression assumes {{c1::linear relationship}} and aims to {{c2::minimize residual sum of squares}}.