---
epoch: 1729351092095
modelKey: gpt-4o-mini|openai
tags:
  - copilot/copilot-conversation
---

**user**: what is BERT in ML
[Timestamp: 2024/10/19 16:18:12]

**ai**: BERT, which stands for Bidirectional Encoder Representations from Transformers, is a state-of-the-art natural language processing (NLP) model developed by Google. It was introduced in a paper published in 2018 and has since become a foundational model for various NLP tasks. Here are some key features of BERT:

1. **Bidirectional Context**: Unlike previous models that processed text in a unidirectional manner (left-to-right or right-to-left), BERT reads text in both directions simultaneously. This allows it to understand the context of a word based on all of its surroundings, leading to better comprehension of language nuances.

2. **Transformers Architecture**: BERT is based on the Transformer architecture, which uses self-attention mechanisms to weigh the importance of different words in a sentence. This enables the model to capture complex relationships between words.

3. **Pre-training and Fine-tuning**: BERT is pre-trained on a large corpus of text using two tasks: masked language modeling (where some words in a sentence are masked and the model learns to predict them) and next sentence prediction (where the model learns to predict if one sentence follows another). After pre-training, BERT can be fine-tuned on specific tasks like sentiment analysis, question answering, or named entity recognition.

4. **Transfer Learning**: BERT's architecture allows it to be fine-tuned for various NLP tasks with relatively small amounts of labeled data, making it a powerful tool for transfer learning in NLP.

5. **Performance**: BERT has achieved state-of-the-art results on a variety of benchmarks and tasks, significantly improving the performance of NLP applications.

Overall, BERT has had a profound impact on the field of NLP, leading to the development of many other models and techniques that build on its principles.
[Timestamp: 2024/10/19 16:18:16]

**user**: what would you call models like BERT and chatGPT and gemini
[Timestamp: 2024/10/19 16:32:52]

**ai**: Models like BERT, ChatGPT, and Gemini can be collectively referred to as **transformer-based language models** or **large language models (LLMs)**. Here’s a breakdown of the terminology:

1. **Transformer Models**: All these models are based on the transformer architecture, which utilizes mechanisms like self-attention to process and generate text. This architecture has revolutionized NLP by enabling models to understand context and relationships in language more effectively.

2. **Large Language Models (LLMs)**: This term specifically refers to models that are trained on vast amounts of text data and have a large number of parameters. They are capable of performing a wide range of language tasks, including text generation, translation, summarization, and more.

3. **Pre-trained Models**: Both BERT and ChatGPT are examples of pre-trained models that can be fine-tuned for specific tasks. BERT is primarily used for understanding tasks, while ChatGPT is designed for generating conversational text.

4. **Generative Models**: ChatGPT and similar models are often categorized as generative models because they can generate coherent and contextually relevant text based on the input they receive. BERT, on the other hand, is more focused on understanding and representation tasks.

5. **AI Conversational Agents**: Models like ChatGPT are also referred to as conversational agents or chatbots, as they are designed to engage in dialogue with users.

In summary, you can refer to BERT, ChatGPT, and Gemini as transformer-based language models or large language models, with specific distinctions based on their functionalities and applications.
[Timestamp: 2024/10/19 16:33:01]

**user**: help me format this text:

Cleaning corpus ie [[Feature Preprocessing]] ie normalisation. That is reducing words to their base or root form, converting to lowercase, and removing stop words

### Code

main normaliser
```python
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import stopwords

def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):
    """Noramlizes data by performing following steps:
        1. Changing each word in corpus to lowercase.
        2. Removing special characters and interpunction.
        3. Dividing text into tokens.
        4. Removing english stopwords.
        5. Stemming words.
        6. Lemmatizing words.
    """
    
    temp = document.lower()
    temp = re.sub(r"[^a-zA-Z0-9]", " ", temp)
    temp = word_tokenize(temp)
    temp = [t for t in temp if t not in stopwords.words("english")]
    temp = [porter_stemmer.stem(token) for token in temp]
    temp = [lemmatizer.lemmatize(token) for token in temp]
    return temp
```

## Processes

[[Tokenisation]]: Used to separate words or sentences.

[[Stemming]]: returns part of a words that doesnt change ie breaks, breakthrough gives break. Use 
```python
from nltk.stem.porter import PorterStemmer
temp=text #decomposed
porter_stemmer = PorterStemmer()
stemmed_tokens = [porter_stemmer.stem(token) for token in temp]

print(stemmed_tokens)
```

[[lemmatization]]:  reducing word to it's base form e.g. words "is", "was", "were" will turn into "be".
```python
from nltk.stem.wordnet import WordNetLemmatizer
temp=text #decomposed
wordnet_lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token, pos="v") for token in temp]
print(lemmatized_tokens)
```

[Timestamp: 2024/10/19 16:41:49]

**ai**: Here’s a formatted version of your text, organized for clarity and readability in Obsidian:

---

# Cleaning Corpus

**Feature Preprocessing**: Normalization involves reducing words to their base or root form, converting them to lowercase, and removing stop words.

## Code

### Main Normalizer
```python
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
from nltk.corpus import stopwords
import re

def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wordnet_lemmatizer):
    """Normalizes data by performing the following steps:
        1. Changing each word in the corpus to lowercase.
        2. Removing special characters and punctuation.
        3. Dividing text into tokens.
        4. Removing English stopwords.
        5. Stemming words.
        6. Lemmatizing words.
    """
    
    temp = document.lower()
    temp = re.sub(r"[^a-zA-Z0-9]", " ", temp)
    temp = word_tokenize(temp)
    temp = [t for t in temp if t not in stopwords.words("english")]
    temp = [porter_stemmer.stem(token) for token in temp]
    temp = [lemmatizer.lemmatize(token) for token in temp]
    return temp
```

## Processes

### [[Tokenisation]]
Used to separate words or sentences.

### [[Stemming]]
Returns part of a word that doesn't change (e.g., "breaks" and "breakthrough" give "break"). Use:
```python
from nltk.stem.porter import PorterStemmer

temp = text  # decomposed
porter_stemmer = PorterStemmer()
stemmed_tokens = [porter_stemmer.stem(token) for token in temp]

print(stemmed_tokens)
```

### [[Lemmatization]]
Reducing a word to its base form (e.g., "is", "was", "were" become "be").
```python
from nltk.stem.wordnet import WordNetLemmatizer

temp = text  # decomposed
wordnet_lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token, pos="v") for token in temp]

print(lemmatized_tokens)
```

---

## [[Feature Preprocessing]]

---
type: 
tags:
  - "#ml"
  - "#ml_optimisation"
  - transformation
---

Feature preprocessing is the initial step of cleaning the data, such as filling in blanks, without dealing with outliers.

In data science, feature preprocessing refers to transforming raw data into a clean and usable format for machine learning models. This step is crucial for improving model performance and ensuring accurate predictions. Key aspects of feature preprocessing include:

1. **Data Cleaning**: Removing or imputing missing values, correcting errors, and handling outliers to ensure data quality.
2. **Normalization and Scaling**: Adjusting the range of features using techniques like min-max scaling or z-score normalization to ensure all features contribute equally to the model.
3. **[[Encoding Categorical Variables]]**: Converting categorical data into numerical format using methods like one-hot encoding or label encoding.
4. **[[Feature Selection]]**: Identifying and retaining the most relevant features that contribute to the predictive power of the model, often using statistical tests or model-based approaches.
5. **[[Dimensionality Reduction]]**: Reducing the number of features while preserving important information, using techniques like Principal Component Analysis (PCA).
6. **[[Feature Engineering]]**: Creating new features from existing data to improve model performance, often based on domain knowledge.

---

## [[Tokenisation]]

---
tags:
  - NLP
---

### Word Tokenisation
```python
from nltk.tokenize import word_tokenize  # keeps punctuation
text_word_tokens_nltk = word_tokenize(text_original)
print(text_word_tokens_nltk)
```

### Sentence Tokenisation
```python
from nltk.tokenize import sent_tokenize
text_sentence_tokens_nltk = sent_tokenize(text_original)
print(text_sentence_tokens_nltk)
```

### Basic Tokenisation
```python
temp = text_original.lower()
temp = re.sub(r"[^a-zA-Z0-9]", " ", temp)  # just letters and numbers
temp = re.sub(r"\[[0-9]+\]", "", temp)  # remove weird stuff
temp = word_tokenize(temp)  # break up text to word list
tokens_no_stopwords = [token for token in temp if token not in stopwords.words("english")]  # remove common words
print(tokens_no_stopwords)
```

---

## [[Stemming]]

---

## [[Lemmatization]]

---
title: 
tags:
  - NLP
aliases
[Timestamp: 2024/10/19 16:42:07]